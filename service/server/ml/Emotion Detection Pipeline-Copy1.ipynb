{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Python Library Imports\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense , Activation , Dropout ,Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.metrics import categorical_accuracy\n",
    "from keras.models import model_from_json\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import *\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage import io\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape (35887, 2304)\n",
      "Y shape (35887,)\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:26: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), input_shape=(48, 48, 1..., padding=\"same\")`\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:33: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (5, 5), padding=\"same\")`\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:40: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), padding=\"same\")`\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:47: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), padding=\"same\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 48, 48, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 48, 48, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 48, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 128)       204928    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 24, 24, 128)       512       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 24, 24, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 12, 12, 512)       590336    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 12, 12, 512)       2048      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 12, 12, 512)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 6, 6, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 6, 6, 512)         2048      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 6, 6, 512)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               1179904   \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 7)                 3591      \n",
      "=================================================================\n",
      "Total params: 4,478,727\n",
      "Trainable params: 4,474,759\n",
      "Non-trainable params: 3,968\n",
      "_________________________________________________________________\n",
      "Train on 32298 samples, validate on 3589 samples\n",
      "Epoch 1/124\n",
      "32298/32298 [==============================] - 406s 13ms/step - loss: 0.3853 - categorical_accuracy: 0.3284 - val_loss: 0.3938 - val_categorical_accuracy: 0.2984\n",
      "Epoch 2/124\n",
      "32298/32298 [==============================] - 464s 14ms/step - loss: 0.3106 - categorical_accuracy: 0.4693 - val_loss: 0.3303 - val_categorical_accuracy: 0.4076\n",
      "Epoch 3/124\n",
      "32298/32298 [==============================] - 409s 13ms/step - loss: 0.2815 - categorical_accuracy: 0.5304 - val_loss: 0.2721 - val_categorical_accuracy: 0.5386\n",
      "Epoch 4/124\n",
      "32298/32298 [==============================] - 405s 13ms/step - loss: 0.2641 - categorical_accuracy: 0.5631 - val_loss: 0.2798 - val_categorical_accuracy: 0.5316\n",
      "Epoch 5/124\n",
      "32298/32298 [==============================] - 412s 13ms/step - loss: 0.2517 - categorical_accuracy: 0.5868 - val_loss: 0.2778 - val_categorical_accuracy: 0.5358\n",
      "Epoch 6/124\n",
      "32298/32298 [==============================] - 396s 12ms/step - loss: 0.2420 - categorical_accuracy: 0.6072 - val_loss: 0.2673 - val_categorical_accuracy: 0.5380\n",
      "Epoch 7/124\n",
      "32298/32298 [==============================] - 390s 12ms/step - loss: 0.2330 - categorical_accuracy: 0.6267 - val_loss: 0.3315 - val_categorical_accuracy: 0.4185\n",
      "Epoch 8/124\n",
      "32298/32298 [==============================] - 424s 13ms/step - loss: 0.2237 - categorical_accuracy: 0.6445 - val_loss: 0.2540 - val_categorical_accuracy: 0.5874\n",
      "Epoch 9/124\n",
      "32298/32298 [==============================] - 438s 14ms/step - loss: 0.2147 - categorical_accuracy: 0.6621 - val_loss: 0.2617 - val_categorical_accuracy: 0.5851\n",
      "Epoch 10/124\n",
      "32298/32298 [==============================] - 395s 12ms/step - loss: 0.2043 - categorical_accuracy: 0.6808 - val_loss: 0.2500 - val_categorical_accuracy: 0.5991\n",
      "Epoch 11/124\n",
      "32298/32298 [==============================] - 504s 16ms/step - loss: 0.1972 - categorical_accuracy: 0.6961 - val_loss: 0.2452 - val_categorical_accuracy: 0.6163\n",
      "Epoch 12/124\n",
      "32298/32298 [==============================] - 472s 15ms/step - loss: 0.1876 - categorical_accuracy: 0.7144 - val_loss: 0.2602 - val_categorical_accuracy: 0.5926\n",
      "Epoch 13/124\n",
      " 6528/32298 [=====>........................] - ETA: 5:48 - loss: 0.1683 - categorical_accuracy: 0.7495"
     ]
    }
   ],
   "source": [
    "# Functions to extract data from csv file and load into a pandas dataframe \n",
    "\n",
    "def load_data(file):\n",
    "    # columns of the data\n",
    "    columns=['emotion','pixels','usage']\n",
    "    # convert the csv data file into a dataframe \n",
    "    df=pd.read_csv(file,names=columns, na_filter=False)\n",
    "    # preview dataframe to see if it was successful\n",
    "    df.head(1)\n",
    "    return df\n",
    "def extract_features_and_labels(df):\n",
    "    X=[] # features/pixels nested array of pixels\n",
    "    Y=[] # Labels\n",
    "    for index, row in df.iterrows():\n",
    "        if(index!=0):\n",
    "            \n",
    "            Y.append(int(row['emotion']))\n",
    "            X.append([int(p) for p in row['pixels'].split()]) # create array of pixel\n",
    "\n",
    "    X, Y = np.array(X) / 255.0, np.array(Y)\n",
    "    return X, Y\n",
    "def cnn_model(num_class):\n",
    "    model = Sequential()\n",
    "\n",
    "    # 1 - Convolution\n",
    "    model.add(Conv2D(64,(3,3), border_mode='same', input_shape=(48, 48,1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    # 2nd Convolution layer\n",
    "    model.add(Conv2D(128,(5,5), border_mode='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    # 3rd Convolution layer\n",
    "    model.add(Conv2D(512,(3,3), border_mode='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    # 4th Convolution layer\n",
    "    model.add(Conv2D(512,(3,3), border_mode='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "    # Flattening\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Fully connected layer 1st layer\n",
    "    model.add(Dense(256))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "    # Fully connected layer 2nd layer\n",
    "    model.add(Dense(512))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(num_class, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[categorical_accuracy])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "def emotion_analysis(emotions):\n",
    "    objects = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n",
    "    \n",
    "    print(\"Predicted Emotion : \", objects[np.argmax(emotions)])\n",
    "    print(\"Probability :\",np.max(emotions))\n",
    "\n",
    "def make_prediction(model):\n",
    "    img = image.load_img('Data/disgust.jpeg', grayscale=True, target_size=(48, 48))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis = 0)\n",
    "    x /= 255\n",
    "\n",
    "    custom = model.predict(x)\n",
    "    print(custom[0])\n",
    "    emotion_analysis(custom[0])\n",
    "    \n",
    "        \n",
    "def ml_pipeline(file):\n",
    "    # labels \n",
    "    labels = ['Anger', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "    # load data \n",
    "    df = load_data(file)\n",
    "    XY = extract_features_and_labels(df)\n",
    "    X, Y = XY\n",
    "    print(\"X shape\", X.shape)\n",
    "    print(\"Y shape\", Y.shape)\n",
    "    num_class = len(set(Y))\n",
    "    print(num_class)\n",
    "    # keras with tensorflow backend\n",
    "    N, D = X.shape\n",
    "    X = X.reshape(N, 48, 48, 1)\n",
    "    \n",
    "    #Split the data, 90% training and 10% testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=0)\n",
    "    y_train = (np.arange(num_class) == y_train[:, None]).astype(np.float32)\n",
    "    y_test = (np.arange(num_class) == y_test[:, None]).astype(np.float32)\n",
    "    \n",
    "    # create model architure \n",
    "    #model=cnn_model(num_class)\n",
    "    K.tensorflow_backend.clear_session() # destroys the current graph and builds a new one\n",
    "    model=cnn_model(num_class)\n",
    "    K.set_value(model.optimizer.lr,1e-3) # set the learning rate\n",
    "    \n",
    "    fitted=model.fit(\n",
    "            x=X_train,     \n",
    "            y=y_train, \n",
    "            batch_size=128,\n",
    "            epochs=124, \n",
    "            verbose=1, \n",
    "            validation_data=(X_test,y_test),\n",
    "            shuffle=True\n",
    "            )\n",
    "    \n",
    "    objects = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n",
    "    y_pos = np.arange(len(objects))\n",
    "    print(y_pos)\n",
    "    \n",
    "    #shape\n",
    "    y_pred=model.predict(X_test)\n",
    "    print(y_pred)\n",
    "    y_test.shape\n",
    "    \n",
    "    make_prediction(model)\n",
    "    #save trained model\n",
    "    joblib.dump(model, \"./production.joblib\", compress=True)\n",
    "    \n",
    "ml_pipeline('Data/data.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "model = joblib.load(\"production.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emotion_analysis(emotions):\n",
    "    objects = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n",
    "    \n",
    "    print(\"Predicted Emotion : \", objects[np.argmax(emotions)])\n",
    "    print(\"Probability :\",np.max(emotions))\n",
    "\n",
    "def make_prediction(model):\n",
    "    img = image.load_img('Data/disgust.jpeg', grayscale=True, target_size=(48, 48))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis = 0)\n",
    "    x /= 255\n",
    "\n",
    "    custom = model.predict(x)\n",
    "    print(custom[0])\n",
    "    emotion_analysis(custom[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_prediction(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
